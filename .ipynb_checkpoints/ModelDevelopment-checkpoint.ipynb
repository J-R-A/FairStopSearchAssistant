{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ccce6ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Imports\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import json\n",
    "import pickle\n",
    "import joblib\n",
    "import category_encoders\n",
    "from random import sample\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.base import TransformerMixin, BaseEstimator \n",
    "from sklearn.preprocessing import FunctionTransformer, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score, cross_validate \n",
    "from sklearn.utils import resample\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, precision_recall_curve, auc, precision_score,ConfusionMatrixDisplay, confusion_matrix\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "#from aux_functs import feat_cap, up_sample, down_sample, feat_filter_drop, create_LocDept, get_hours, CatAges, LocationTransformer, NanImputer\n",
    "from aux_functs import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff47a22f",
   "metadata": {},
   "source": [
    "# Obectives\n",
    "\n",
    "## Client requirements \n",
    "\n",
    "The client wants us to develop a model to help police officers deciding on when to search, or not, a stopped vehicle. The police wants to find the largest possible amount of contraband but, being a search a disruptive event for the citizens, demands for contraband to be found in at least half of the conducted searches.\n",
    "\n",
    "The client also demands for our model to be equally equally successful between specific protected class groups within a 5% error limit.\n",
    "\n",
    "\n",
    "## Requirements clarification\n",
    "\n",
    "\n",
    "### Performance\n",
    "The client wants to maximize the amount of discovered contraband while minimizing the amount of searches needed to do so. **Precision** (contraband found / searches performed) and **recall** ( contraband found / total contraband present) are the adequate metrics to address and quantify these requirements. \n",
    "\n",
    "There’s a natural trade-off between precision and recall, if we want to be very sure (precise) about our predictions we tend to be more picky and risk uncover less contraband. Our final aim will be to find contraband in at least half of our contraband predictions **( precision of at least 50%)** and to find at least half of the contraband present **( recall of at least 50% )**.\n",
    "\n",
    "\n",
    "### Fairness\n",
    "\n",
    "To avoid discrimination against protected classes the department wants the search sucess for the groups of these classes to be **within a 5% limit in all departments**. We are going to use **precision** as the metric to compare the groups within each class. A lower precision from one of the classes implies a higher number of false positives and so a discriminatory tendency from our model towards that class. We’ll aim to try to have, in each sub-department, all groups within the same protected classe with a **precision score within a 5 percentage points margin**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af3da5a3",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Data/FairSearchData.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/8g/f9kv512n5c14r66nv87r75v40000gp/T/ipykernel_15640/3408581846.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'Data/FairSearchData.csv'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mfair_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m ''' Because we are interested in predicting which vehicles have contraband we are just going to use observations\n",
      "\u001b[0;32m~/miniforge3/envs/fair/lib/python3.8/site-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/fair/lib/python3.8/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/fair/lib/python3.8/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/fair/lib/python3.8/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    809\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 811\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    812\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/fair/lib/python3.8/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1038\u001b[0m             )\n\u001b[1;32m   1039\u001b[0m         \u001b[0;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1040\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/fair/lib/python3.8/site-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;31m# open handles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/fair/lib/python3.8/site-packages/pandas/io/parsers/base_parser.py\u001b[0m in \u001b[0;36m_open_handles\u001b[0;34m(self, src, kwds)\u001b[0m\n\u001b[1;32m    220\u001b[0m         \u001b[0mLet\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mreaders\u001b[0m \u001b[0mopen\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0mafter\u001b[0m \u001b[0mthey\u001b[0m \u001b[0mare\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtheir\u001b[0m \u001b[0mpotential\u001b[0m \u001b[0mraises\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m         \"\"\"\n\u001b[0;32m--> 222\u001b[0;31m         self.handles = get_handle(\n\u001b[0m\u001b[1;32m    223\u001b[0m             \u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m             \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/fair/lib/python3.8/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    700\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    701\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 702\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    703\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Data/FairSearchData.csv'"
     ]
    }
   ],
   "source": [
    "### Load Data \n",
    "\n",
    "filename = 'Data/FairSearchData.csv'\n",
    "fair_df = pd.read_csv(filename)\n",
    "\n",
    "''' Because we are interested in predicting which vehicles have contraband we are just going to use observations\n",
    "where a search was performed to develop our model, in the other observations we have no way of knowing if \n",
    "contraband was present or not.''' \n",
    "\n",
    "fair_df = fair_df[fair_df['Vehicle Searched Indicator'] == True] # Filter DF\n",
    "fair_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a27c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Perfrom train test split and resample\n",
    "\n",
    "\n",
    "## Train test split\n",
    "\n",
    "#separate features and target variables. Our target variable is the feature 'Contraband Indicator'  \n",
    "X, y = fair_df.drop('Contraband Indicator', axis=1), fair_df[['Contraband Indicator']].astype('bool')\n",
    "\n",
    "#split Features and Target in train and test sets  \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, shuffle = False)\n",
    "\n",
    "\n",
    "## Resample\n",
    "\n",
    "'''Because our categories are very umbalanced, there are much more observations where contraband was not found\n",
    "than the ones where its was, we are going to downnsample the category with more observations.'''   \n",
    "\n",
    "#downsample\n",
    "X_train_rs, y_train_rs = down_sample(X_train, y_train)\n",
    "\n",
    "\n",
    "## Data description and check\n",
    "c_obs = (y_train_rs['Contraband Indicator'] == True).sum() # contraband obs\n",
    "nc_obs = (y_train_rs['Contraband Indicator'] == False).sum() # no contraband obs\n",
    "\n",
    "output = (f'After splits and downsampling, X_train has: {X_train_rs.shape[1]} features and {X_train_rs.shape[0]}'\n",
    "          f' observations;\\n'\n",
    "          f'X_test has: {X_test.shape[1]} features and {X_test.shape[0]} observations;\\n'\n",
    "          f'y_train has: {1} features and {y_train_rs.shape[0]} observations - {c_obs} contraband, ' \n",
    "          f'{nc_obs} no contraband;\\n'\n",
    "          f'y_test has: {1} features and {y_test.shape[0]} observations. ')\n",
    "print(output)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cc2a9bd",
   "metadata": {},
   "source": [
    "# Features engineering and model selection\n",
    "\n",
    "We'll use a pipeline to transform the features in the data set and fit the model.\n",
    "\n",
    "\n",
    "## Features engineering\n",
    "\n",
    "We'll transform our data-set trough a series of transformers applied in the pipeline. A description of each transformation follows:\n",
    "\n",
    "### FeatFilterDrop:\n",
    "\n",
    "\n",
    "1. Drop selected features:<br>\n",
    "    a. Features related with protected groups - 'Subjec Race Code', 'Subjec Sex Code', 'Subjec ethnicity Code' - as we don't want the model to take them into account.<br>\n",
    "    b.'Vehicle Searched Undicator' - as explained above we are only going to use observations from searched vehicles.<br>\n",
    "    c. 'Reporting Officer Identification ID' - too many unique values and not clear how the id of officer stopping the vehicle would determine if it has contraband or not \n",
    "\n",
    "2. Filter 'Subject Age' feature so that all non numeric values or ages higher or equal than 100 are set to NaN. Change the data type to float.\n",
    "\n",
    "\n",
    "### NanImputer :\n",
    "\n",
    "1. Inputes the mode of the corresponding feature to all NaN values in the data set.\n",
    "\n",
    "\n",
    "### CreateLocDept:\n",
    "\n",
    "The features 'Department Name' and 'Intervention Location Name' frequently have the same value so we are going to coombine them creating a *new features* named 'LocDept'.\n",
    "\n",
    "1. Create 'LocDept' feature:<br>\n",
    "    a. Lower case both original features and strip them of spaces.<br>\n",
    "    b. If the values in 'Department Name' and 'Intervention Location Name' are the same 'LocDept' is set as that; if not the value of 'LocDept' results from the concatenation of the original values.<br>\n",
    "    c. Drop 'Department Name' and 'Intervention Location Name'.<br>\n",
    "    \n",
    "\n",
    "### GetHours:\n",
    "\n",
    "To use cyclical features, like hours, it's best practice to encode them under a sine and a cosyne transformations that reflect their cyclical nature. Otherwise the model would incorrectly interpret 12:00 and 01:00 AM to be much more different than 02:00 and 03:00 AM, for instance, despite that in both cases the values are separated by the same amount of time.\n",
    "\n",
    "1. Extract the hour of the day from the 'Intervention DateTime' feature.\n",
    "2. Apply sine and cosine transformations to the hours and create *new features* 's_hours' and 'c_hours' to store these.\n",
    "3. Drop 'Intervention DateTime' feature.\n",
    "\n",
    "\n",
    "### CatAges:\n",
    "\n",
    "1. Bin the ages in the 'Subject Age' feature in 5 years bins from 0 to 100.\n",
    "2. Change observation data type to string.\n",
    "\n",
    "\n",
    "### FeatCap:\n",
    "\n",
    "1. Cast to string, lower case and strip of spaces the observations in the features 'Intervention Reason Code','Statute Code Description','Search Authorization Code'\n",
    "\n",
    "\n",
    "### LocationTransformer:\n",
    "\n",
    "After the transformations we are going to *one hot encode* all our features. To reduce the dimensionality of our final data set we are going to reduce the number of unique observations in the 'LocDept' feature.\n",
    "\n",
    "1. Find the *n* 'LocDept' labels, with more than 50 observations, where a bigguer proportion of contraband was found. The *n* used was 75.\n",
    "2. Keep those and convert all others to the label *'other'*\n",
    "\n",
    "\n",
    "\n",
    "## Model\n",
    "\n",
    "We didn't perform a true model selection. Our requirements, the sensitivity of the question at hand, and the fact that we considered this a first exploratory approach strongly favoured a simple model with a good degree of interpretability and transparency. Because of these our preference went to a logistic regression model.\n",
    "\n",
    "We performed a grid search, with cross validation optimizing the metric 'precision', to determine the best parameters for the model: *'C'* that deternines the strength of the refularization, and the type of regularization *'l1'* (Lasso) or *'l2'* (Ridge).\n",
    "\n",
    "The chosen parameters were:<br>\n",
    "        . C - 0.1<br>\n",
    "        . Penalty - 'l1'<br>\n",
    "        \n",
    "        \n",
    "        \n",
    "*(**Note:** We also tried both a decision treee and a random forest classifier, cross-validating the max depth parameter. The peformance of both, in terms of precision and recall, was worst or only slightly better than the logistic regression model, as such, and for the reasons explained above, we stuck with our initial choice.*   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2d654a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Fit Model\n",
    "\n",
    "#model parameters to cross validate \n",
    "params = {'C': [0.01, 0.05, 0.1, 0.5, 1, 1.5],'penalty':('l1','l2')}\n",
    "\n",
    "#build pipeline\n",
    "pipeline = make_pipeline(FunctionTransformer(FeatFilterDrop,validate=False),\n",
    "                          NanImputer(),\n",
    "                          FunctionTransformer(CreateLocDept,validate=False),\n",
    "                          FunctionTransformer(GetHours,validate=False),\n",
    "                          FunctionTransformer(CatAges,validate=False),\n",
    "                          FunctionTransformer(FeatCap,validate=False),\n",
    "                          LocationTransformer(),\n",
    "                          category_encoders.OneHotEncoder(handle_unknown='ignore'),\n",
    "                          GridSearchCV(estimator = LogisticRegression(solver = 'saga'), \n",
    "                          param_grid = params,\n",
    "                          cv=5, \n",
    "                          scoring=\"precision\",\n",
    "                          return_train_score=True,\n",
    "                          refit=True,\n",
    "                          error_score='raise'))\n",
    "\n",
    "#fit model\n",
    "pipeline.fit(X_train_rs,y_train_rs)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3222a0f5",
   "metadata": {},
   "source": [
    "# Model Performance\n",
    "\n",
    "### After fitting the model in the training set we are going to test it in the held out test set, to form an idea of how it would perform in new unseen real data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081447f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Predict from test split\n",
    "\n",
    "#get predictions with 0.5 probability treshold\n",
    "preds = pipeline.predict(X_test)\n",
    "\n",
    "#create Df with the contraband predictions and true labels \n",
    "preds_Df = pd.DataFrame(columns = ['Preds','True_Id'])\n",
    "preds_Df['Preds'] = pd.Series(preds)\n",
    "preds_Df['True_Id'] = y_test['Contraband Indicator'].reset_index(drop=True)\n",
    "\n",
    "\n",
    "#get the predicted probabilities per observation\n",
    "prob_y_0 = pipeline.predict_proba(X_test)\n",
    "prob_y_0 = [p[1] for p in prob_y_0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b78e6d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Calculate and plot model performance\n",
    "\n",
    "## Calculate Performance metrics\n",
    "\n",
    "#confusion matrix\n",
    "conf_mat = confusion_matrix(preds_Df['True_Id'], preds_Df['Preds'])\n",
    "\n",
    "hits = conf_mat[1,1] # Contrabands correctly predicted\n",
    "positives = conf_mat[1].sum() # Contrabands on the real data \n",
    "\n",
    "f_alarms = conf_mat[0,1] # Contrabands incorrectly predicted\n",
    "negatives = conf_mat[0].sum() # Non contrabands on real the data\n",
    "\n",
    "\n",
    "#precision-recall\n",
    "precision = hits / (hits + f_alarms)\n",
    "recall = hits / positives\n",
    "\n",
    "\n",
    "#precision-recall curve\n",
    "lr_precision, lr_recall, tresh2 = precision_recall_curve(preds_Df['True_Id'], prob_y_0) # Precision recall curve\n",
    "pr_AUC = auc(lr_recall, lr_precision) # Area under precisition recall curve\n",
    "pr_no_skill = preds_Df[preds_Df['True_Id']==True].shape[0] / preds_Df['True_Id'].shape[0] # line of no skill for precisition recall curve\n",
    "\n",
    "\n",
    "#ROC curve\n",
    "roc_AUC = roc_auc_score(preds_Df['True_Id'], prob_y_0) # ROC auc score\n",
    "lr_fpr, lr_tpr, tresh1 = roc_curve(preds_Df['True_Id'], prob_y_0) # ROC auc curve\n",
    "\n",
    "\n",
    "\n",
    "## Plot model performance\n",
    "\n",
    "#define fig and subplots\n",
    "fig,axs = plt.subplots(2, 2, figsize=(12,12))\n",
    "\n",
    "#plots font sizes\n",
    "l_font = 15\n",
    "t_font = 20\n",
    "txt_font = 12\n",
    "\n",
    "\n",
    "#plot confusion matrix\n",
    "ConfusionMatrixDisplay.from_predictions(preds_Df['True_Id'],preds_Df['Preds'],\n",
    "                                        ax=axs[0,0],\n",
    "                                        display_labels=[\"no\", \"yes\"],\n",
    "                                        normalize='true',\n",
    "                                        cmap='Blues')\n",
    "axs[0,0].set_title('Confusion Matrix',fontsize = t_font)\n",
    "axs[0,0].set_xlabel('Predicted Contraband',fontsize = l_font)\n",
    "axs[0,0].set_ylabel('Contraband',fontsize = l_font)\n",
    "\n",
    "\n",
    "#plot precision and recall for 0.5 probability treshold\n",
    "y1 = [precision*100, recall*100]\n",
    "axs[0,1].bar(['Precision', 'Recall'],y1,color=['#B3B3B3'])\n",
    "axs[0,1].set_ylabel('Percentage %',fontsize = l_font)\n",
    "axs[0,1].set_title('Relevant Metrics',fontsize = t_font)\n",
    "axs[0,1].set_ylim([0, 100])\n",
    "axs[0,1].set_xlim([-0.75, 1.75])\n",
    "\n",
    "for i, v in enumerate(y1): # place values on top of bars\n",
    "    axs[0,1].text(i - 0.15, v + 0.8, str(np.round(v,2)),fontsize = txt_font)\n",
    "    \n",
    "    \n",
    "#plot precision-recall curve\n",
    "axs[1,0].plot(lr_recall,lr_precision,color='#E52123')\n",
    "axs[1,0].plot([0, 1], [pr_no_skill, pr_no_skill], linestyle=':', label='No Skill', color = '#B3B3B3')\n",
    "axs[1,0].text(0.02,pr_no_skill+0.01, 'No Skill',fontsize = txt_font, color = '#B3B3B3')\n",
    "axs[1,0].text(0.7,0.9, 'AUC='+str(np.round(pr_AUC,2)),fontsize = l_font, color = '#E52123')\n",
    "axs[1,0].set_xlabel('Recall',fontsize = l_font)\n",
    "axs[1,0].set_ylabel('Precision',fontsize = l_font)\n",
    "axs[1,0].set_title('Precision - Recall Curve',fontsize = t_font)\n",
    "axs[1,0].set_ylim([0.35, 1])\n",
    "axs[1,0].set_xlim([0, 1])\n",
    "\n",
    "\n",
    "#plot ROC curve\n",
    "axs[1,1].plot(lr_fpr,lr_tpr,color='#E52123')\n",
    "axs[1,1].plot([0, 1], [0, 1], linestyle=':', label='No Skill', color = '#B3B3B3')\n",
    "axs[1,1].text(0.2,0.24, 'No Skill',fontsize = txt_font, color = '#B3B3B3',rotation = 45)\n",
    "axs[1,1].text(0.7,0.9, 'AUC='+str(np.round(roc_AUC,2)),fontsize = l_font, color = '#E52123')\n",
    "axs[1,1].set_xlabel('FP Rate',fontsize = l_font)\n",
    "axs[1,1].set_ylabel('TP Rate',fontsize = l_font)\n",
    "axs[1,1].set_title('ROC Curve',fontsize = t_font)\n",
    "axs[1,1].set_ylim([0, 1])\n",
    "axs[1,1].set_xlim([0, 1])\n",
    "\n",
    "\n",
    "fig.tight_layout(pad=3.0)\n",
    "\n",
    "plt.show()    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b08bc3",
   "metadata": {},
   "source": [
    "## Performance Results\n",
    "\n",
    "Looking at the confusion matrix (top left) we can see that our model correctly classifies 0.71 of the contraband observations (True Positives) and 0.65 of the observations with no contraband (True Negatives). \n",
    "\n",
    "The model is well within the initially established requirements. Our objectives were a **precision** and a **recall** of at least 50%, has shown in the top right plot the model acomplished a recall of 71.39% and a precision of 56.17%.\n",
    "\n",
    "The metrics above are the ones obtained using a probabilities treshold of 0.5 to determine our predictions from the model aasigned probabilities. To get a measure of performance that is not influenced by the choice of treshold we can look at the **Precison-Recall** and **ROC** curves in the bottom plots: both are well above the respective *No Skill* lines. \n",
    "\n",
    "By looking at the **Precision-Recall** curve we can also see that by choosing a different treshold we would have been able to obtain different, above 50%, pairs of values for the metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "659417b1",
   "metadata": {},
   "source": [
    "# Model Fairness\n",
    "\n",
    "Our model seems to fulfill the client's requirement in terms of the amount of contaband uncovered and the searches needed to do so. But the police was also worried about the way the model would treat different protected classes. A difference in precision less or equal than 5%, between protected classe's groups, in each department, was defined as an objective. \n",
    "\n",
    "\n",
    "## General fairness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b85012c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "### General bias analysis\n",
    "\n",
    "\n",
    "#create data frame with relevant data \n",
    "df_1 = X_test[['Department Name','Subject Race Code','Subject Ethnicity Code','Subject Sex Code']].reset_index(drop=True)\n",
    "df_2 = preds_Df.reset_index(drop=True)\n",
    "bias_df = pd.concat([df_1,df_2],axis = 1)\n",
    "\n",
    "\n",
    "## Calculate and organize data to plot\n",
    "\n",
    "#race bias \n",
    "races = ['W','B']\n",
    "r_dict = {}\n",
    "for r in races:\n",
    "    r_dict[r] = {}\n",
    "    f = bias_df['Subject Race Code'] == r\n",
    "    r_dict[r]['df'] = bias_df.loc[f,['Subject Race Code','Preds','True_Id']]\n",
    "    conf_mat = confusion_matrix(r_dict[r]['df']['True_Id'], r_dict[r]['df']['Preds'])\n",
    "    r_dict[r]['hits'] = conf_mat[1,1]\n",
    "    r_dict[r]['pos'] = conf_mat[1].sum() \n",
    "    r_dict[r]['fa'] = conf_mat[0,1] \n",
    "    r_dict[r]['neg'] = conf_mat[0].sum()\n",
    "    r_dict[r]['precision'] = r_dict[r]['hits'] /(r_dict[r]['hits'] + r_dict[r]['fa'])\n",
    "    \n",
    "    \n",
    "#ethnicity bias \n",
    "ethnicities = ['H','N']\n",
    "e_dict = {}\n",
    "for e in ethnicities:\n",
    "    e_dict[e] = {}\n",
    "    f = bias_df['Subject Ethnicity Code'] == e\n",
    "    e_dict[e]['df'] = bias_df.loc[f,['Subject Ethnicity Code','Preds','True_Id']]\n",
    "    conf_mat = confusion_matrix(e_dict[e]['df']['True_Id'],e_dict[e]['df']['Preds'])\n",
    "    e_dict[e]['hits'] = conf_mat[1,1]\n",
    "    e_dict[e]['pos'] = conf_mat[1].sum() \n",
    "    e_dict[e]['fa'] = conf_mat[0,1] \n",
    "    e_dict[e]['neg'] = conf_mat[0].sum()\n",
    "    e_dict[e]['precision'] = e_dict[e]['hits'] /(e_dict[e]['hits'] + e_dict[e]['fa'])\n",
    "    \n",
    "    \n",
    "    \n",
    "#gender bias \n",
    "genders = ['M','F']\n",
    "g_dict = {}\n",
    "for g in genders:\n",
    "    g_dict[g] = {}\n",
    "    f = bias_df['Subject Sex Code'] == g\n",
    "    g_dict[g]['df'] = bias_df.loc[f,['Subject Sex Code','Preds','True_Id']]\n",
    "    conf_mat = confusion_matrix(g_dict[g]['df']['True_Id'], g_dict[g]['df']['Preds'])\n",
    "    g_dict[g]['hits'] = conf_mat[1,1]\n",
    "    g_dict[g]['pos'] = conf_mat[1].sum() \n",
    "    g_dict[g]['fa'] = conf_mat[0,1] \n",
    "    g_dict[g]['neg'] = conf_mat[0].sum()\n",
    "    g_dict[g]['precision'] = g_dict[g]['hits'] /(g_dict[g]['hits'] + g_dict[g]['fa'])    \n",
    "\n",
    "\n",
    "\n",
    "## Plot general bias data \n",
    "\n",
    "#define fig and subplots\n",
    "fig,axs = plt.subplots(3, 3, figsize=(18,18))\n",
    "\n",
    "#plots font sizes\n",
    "l_font = 15\n",
    "t_font = 20\n",
    "txt_font = 12\n",
    "\n",
    "\n",
    "dicts = [r_dict,e_dict,g_dict] # data to plot\n",
    "\n",
    "#subplot initial indices \n",
    "cx = 0\n",
    "rx = 0\n",
    "\n",
    "p_groups = ['Race','Ethnicity','Gender'] # plots labels\n",
    "g_clrs = ['#4899D3','#91C090','#D79044'] # plots colors\n",
    "\n",
    "for d in dicts:\n",
    "    #plot confusion matrices\n",
    "    for r in d.keys():\n",
    "        \n",
    "        ConfusionMatrixDisplay.from_predictions(d[r]['df']['True_Id'],d[r]['df']['Preds'],\n",
    "                                                ax=axs[rx,cx],\n",
    "                                                display_labels=[\"no\", \"yes\"],\n",
    "                                                normalize='true',\n",
    "                                                cmap='Blues',\n",
    "                                                colorbar = True)\n",
    "      \n",
    "        axs[rx,cx].set_title(p_groups[cx]+' '+ r,fontsize = t_font)\n",
    "        axs[rx,cx].set_xlabel('Predicted Contraband',fontsize = l_font)\n",
    "        axs[rx,cx].set_ylabel('Contraband',fontsize = l_font)\n",
    "        \n",
    "        #update subplot indices\n",
    "        if rx == 0:\n",
    "            rx = 1\n",
    "        else:\n",
    "            rx = 0\n",
    "    \n",
    "    \n",
    "    #plot precisions\n",
    "    g_labels = list(d.keys())\n",
    "    y1 = [d[g_labels[0]]['precision']*100, d[g_labels[1]]['precision']*100]\n",
    "    pp_diff = np.round(np.abs(d[g_labels[0]]['precision']*100 - d[g_labels[1]]['precision']*100),1)\n",
    "    axs[2,cx].bar([g_labels[0], g_labels[1]],y1,color=g_clrs[cx])\n",
    "    axs[2,cx].text(1, 85, 'Diff: '+str(pp_diff)+' p.p',fontsize = l_font,color=g_clrs[cx])\n",
    "    axs[2,cx].set_xlabel(p_groups[cx]+' '+'groups',fontsize = l_font)\n",
    "    axs[2,cx].set_ylabel('Percentage %',fontsize = l_font)\n",
    "    axs[2,cx].set_title(p_groups[cx]+' '+ 'Precision',fontsize = t_font)\n",
    "    axs[2,cx].set_ylim([0, 100])\n",
    "    axs[2,cx].set_xlim([-0.75, 1.75])\n",
    "          \n",
    "    for i, v in enumerate(y1): # place values on top of bars\n",
    "        axs[2,cx].text(i - 0.15, v + 0.8, str(np.round(v,2)),fontsize = txt_font)\n",
    "    \n",
    "    #update subplot indices\n",
    "    cx = cx+1\n",
    "\n",
    "\n",
    "    \n",
    "fig.tight_layout(pad=3.0)       \n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d114955",
   "metadata": {},
   "source": [
    "## General fairness results\n",
    "\n",
    "A glance at the confusion matrices shows us that the results of the model are qualitatively consistent across the different protected classes' groups and also with the general results presented above.\n",
    "\n",
    "In the bottom plots we can see that that from the three protected classes only in race the different in precision between the two different groupd excedes the 5 p.p., that was our objective, buth not by much. Both it the gender and ethnicity groups the differences are smaller.\n",
    "\n",
    "\n",
    "## Fairness by department "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "844f8392",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Calculate and plot precision differences by department\n",
    "\n",
    "\n",
    "## Calculations\n",
    "\n",
    "#get dept names \n",
    "obs_dpt = bias_df['Department Name'].value_counts()\n",
    "dpt_names = obs_dpt.keys().tolist()\n",
    "\n",
    "#only include depts wit #obs above this treshold\n",
    "obs_tresh = 25 \n",
    "\n",
    "#df's to store the data\n",
    "r_precision = pd.DataFrame()\n",
    "e_precision = pd.DataFrame()\n",
    "g_precision = pd.DataFrame()\n",
    "t_precision = pd.DataFrame()\n",
    "\n",
    "#for each department\n",
    "for dpt in dpt_names:\n",
    "    \n",
    "    #precision per dept\n",
    "    t_f = bias_df['Department Name'] == dpt\n",
    "    t_df = bias_df.loc[t_f,['Preds','True_Id']]\n",
    "    if t_df.shape[0] >= obs_tresh:\n",
    "        conf_mat = confusion_matrix(t_df['True_Id'],t_df['Preds'])\n",
    "        t_hits = conf_mat[1,1]\n",
    "        t_pos = conf_mat[1].sum() \n",
    "        t_fa = conf_mat[0,1]\n",
    "        if t_hits != 0:\n",
    "            t_precision.loc[dpt,'total'] = (t_hits /(t_hits + t_fa))*100\n",
    "        else:\n",
    "            t_precision.loc[dpt,g] = 0\n",
    "    \n",
    "    \n",
    "    \n",
    "    #race precision per dept\n",
    "    for r in ['W','B']:\n",
    "        r_f = (bias_df['Department Name'] == dpt) & (bias_df['Subject Race Code'] == r)\n",
    "        r_df = bias_df.loc[r_f,['Preds','True_Id']]\n",
    "        conf_mat = confusion_matrix(r_df['True_Id'],r_df['Preds'])\n",
    "        if r_df.shape[0] >= obs_tresh:\n",
    "            r_hits = conf_mat[1,1]\n",
    "            r_pos = conf_mat[1].sum() \n",
    "            r_fa = conf_mat[0,1]\n",
    "            if r_hits != 0: \n",
    "                r_precision.loc[dpt,r] = (r_hits /(r_hits + r_fa))*100\n",
    "            else:\n",
    "                r_precision.loc[dpt,r] = 0\n",
    "    \n",
    "    r_precision['diff'] = np.abs(r_precision['W'] - r_precision['B'])\n",
    "    \n",
    "   \n",
    "    #ethnicity precision per dept\n",
    "    for e in ['H','N']:\n",
    "        e_f = (bias_df['Department Name'] == dpt) & (bias_df['Subject Ethnicity Code'] == e)\n",
    "        e_df = bias_df.loc[e_f,['Preds','True_Id']]\n",
    "        conf_mat = confusion_matrix(e_df['True_Id'],e_df['Preds'])\n",
    "        if e_df.shape[0] >= obs_tresh:\n",
    "            e_hits = conf_mat[1,1]\n",
    "            e_pos = conf_mat[1].sum() \n",
    "            e_fa = conf_mat[0,1]\n",
    "            if e_hits != 0:\n",
    "                e_precision.loc[dpt,e] = (e_hits /(e_hits + e_fa))*100\n",
    "            else:\n",
    "                e_precision.loc[dpt,e] = 0\n",
    "    \n",
    "    e_precision['diff'] = np.abs(e_precision['H'] - e_precision['N'])\n",
    "    \n",
    "    \n",
    "    #gender precision per dept\n",
    "    for g in ['M','F']:\n",
    "        g_f = (bias_df['Department Name'] == dpt) & (bias_df['Subject Sex Code'] == g)\n",
    "        g_df = bias_df.loc[g_f,['Preds','True_Id']]\n",
    "        conf_mat = confusion_matrix(g_df['True_Id'],g_df['Preds'])\n",
    "        if g_df.shape[0] >= obs_tresh:\n",
    "            g_hits = conf_mat[1,1]\n",
    "            g_pos = conf_mat[1].sum() \n",
    "            g_fa = conf_mat[0,1]\n",
    "            if g_hits != 0:\n",
    "                g_precision.loc[dpt,g] = (g_hits /(g_hits + g_fa))*100\n",
    "            else:\n",
    "                g_precision.loc[dpt,g] = 0\n",
    "    \n",
    "    g_precision['diff'] = np.abs(g_precision['M'] - g_precision['F'])\n",
    "\n",
    "\n",
    "## Plot precision differences in protected classes\n",
    "    \n",
    "#define fig and subplots\n",
    "fig,axs = plt.subplots(2, 2, figsize=(10,10))\n",
    "\n",
    "#plots font sizes\n",
    "l_font = 15\n",
    "t_font = 20\n",
    "txt_font = 12\n",
    "\n",
    "\n",
    "#plot precision per department\n",
    "axs[0,0].hist(t_precision['total'],bins = 20, color=['#B3B3B3'])\n",
    "tp_mean = t_precision['total'].mean()\n",
    "tp_std = t_precision['total'].std()\n",
    "axs[0,0].plot([tp_mean, tp_mean], [0, 15.5], linestyle=':', color =  '#E52123')\n",
    "axs[0,0].plot([tp_mean - tp_std, tp_mean + tp_std], [6, 6], linestyle=':', color =  '#E52123')\n",
    "\n",
    "axs[0,0].text(tp_mean+1, 14.5,\n",
    "              'Mean: '+str(np.round(tp_mean,2)),\n",
    "              fontsize = txt_font,\n",
    "              color =  '#E52123')\n",
    "\n",
    "axs[0,0].text(tp_mean+1.5, 5.2,\n",
    "              'std: '+str(np.round(tp_std,2)),\n",
    "              fontsize = txt_font,\n",
    "              color =  '#E52123')\n",
    "\n",
    "axs[0,0].set_ylabel('# Depts',fontsize = l_font)\n",
    "axs[0,0].set_xlabel('Precision',fontsize = l_font)\n",
    "axs[0,0].set_title('Depts Precision',fontsize = t_font)\n",
    "axs[0,0].set_ylim([0, 15.5])\n",
    "axs[0,0].set_xlim([0, 101])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#plot race differences\n",
    "axs[0,1].hist(r_precision['diff'],bins = 50, color=['#4899D3'])\n",
    "\n",
    "t_off = 0\n",
    "for t in [5,10,20]:\n",
    "    valid_df = r_precision.dropna().copy()\n",
    "    val = np.round(valid_df.loc[valid_df['diff']<=t,'diff'].shape[0] / valid_df.shape[0]*100,1)\n",
    "    axs[0,1].text(20, 9-t_off,\n",
    "              str(val)+'% diff <= '+ str(t),\n",
    "              fontsize = txt_font,\n",
    "              color =  '#4899D3')\n",
    "    t_off = t_off+0.6\n",
    "    \n",
    "axs[0,1].set_ylabel('# Depts',fontsize = l_font)\n",
    "axs[0,1].set_xlabel('Abs Precision Diff',fontsize = l_font)\n",
    "axs[0,1].set_title('Race (W,B) Precision Diff',fontsize = t_font)\n",
    "axs[0,1].set_ylim([0, 10])\n",
    "axs[0,1].set_xlim([0, 40])\n",
    "\n",
    "\n",
    "#plot ethnicity differences \n",
    "axs[1,0].hist(e_precision['diff'],bins = 50, color=['#91C090'])\n",
    "\n",
    "t_off = 0\n",
    "for t in [5,10,20]:\n",
    "    valid_df = e_precision.dropna().copy()\n",
    "    val = np.round(valid_df.loc[valid_df['diff']<=t,'diff'].shape[0] / valid_df.shape[0]*100,1)\n",
    "    axs[1,0].text(20, 13.5-t_off,\n",
    "              str(val)+'% diff <= '+ str(t),\n",
    "              fontsize = txt_font,\n",
    "              color =  '#91C090')\n",
    "    t_off = t_off+0.9\n",
    "    \n",
    "axs[1,0].set_ylabel('# Depts',fontsize = l_font)\n",
    "axs[1,0].set_xlabel('Abs Precision Diff',fontsize = l_font)\n",
    "axs[1,0].set_title('Ethnicity (H,N) Precision Diff',fontsize = t_font)\n",
    "axs[1,0].set_ylim([0, 15])\n",
    "axs[1,0].set_xlim([0, 40])\n",
    "\n",
    "\n",
    "#plot gender differences \n",
    "axs[1,1].hist(g_precision['diff'],bins = 50, color=['#D79044'])\n",
    "\n",
    "t_off = 0\n",
    "for t in [5,10,20]:\n",
    "    valid_df = g_precision.dropna().copy()\n",
    "    val = np.round(valid_df.loc[valid_df['diff']<=t,'diff'].shape[0] / valid_df.shape[0]*100,1)\n",
    "    axs[1,1].text(20, 9-t_off,\n",
    "              str(val)+'% diff <= '+ str(t),\n",
    "              fontsize = txt_font,\n",
    "              color =  '#D79044')\n",
    "    t_off = t_off+0.6\n",
    "    \n",
    "axs[1,1].set_ylabel('# Depts',fontsize = l_font)\n",
    "axs[1,1].set_xlabel('Abs Precision Diff',fontsize = l_font)\n",
    "axs[1,1].set_title('Gender (M,F) Precision Diff',fontsize = t_font)\n",
    "axs[1,1].set_ylim([0, 10])\n",
    "axs[1,1].set_xlim([0, 40])\n",
    "\n",
    "\n",
    "fig.tight_layout(pad=3.0)       \n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cbf8ae0",
   "metadata": {},
   "source": [
    "## Fairness by department results\n",
    "\n",
    "The first important observation is that there is a considerable variability in overall precision per department (top left), std = 15.08.\n",
    "\n",
    "The second is that the objective of keeping the groups of all protected classes within a 5 p.p precision, in all departments, was not acomplished. As can be seen above, in all classes, roughly more than thirty, fifty and eighty percent of the departmens have their groups within a 5, 10 and 20 percentage points margin. Remember that this is without feeding the model with any explicit information about the protected classes.\n",
    "\n",
    "To deal with such we tried several resampling strategies, both at the global and department level, so that the different groups of the protected classes had the same number of observarions with and without contraband in the training set. None of these were successful, with the impact on the difference of precision between groups being minimal, and we decided not to implement them in the final version of the model. \n",
    "\n",
    "We didn't include in these analysis departments with less than 25 observations.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1825f8ab",
   "metadata": {},
   "source": [
    "# Feature importance\n",
    "\n",
    "One of the reasons for which we decided to use a logistic regression model was its interpretability. We can use the coefficients attributed to each predictive feature to know its importance in the ability of the model to make predictions. In our case this can be done directly because, being one hot encoded, all features were in the same units scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b4070f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Plot features' coefficients\n",
    "\n",
    "## Get coefs\n",
    "\n",
    "#coefs for best estimator\n",
    "coefs = pipeline[-1].best_estimator_.coef_[0]\n",
    "#coefs names\n",
    "feat_names = pipeline[-1].best_estimator_.feature_names_in_\n",
    "\n",
    "#sorted coefs and names\n",
    "sorting_order = np.argsort(np.abs(coefs)*-1)\n",
    "sorted_coefs = coefs[sorting_order] # Sorted Coefs\n",
    "sorted_names = feat_names[sorting_order] # Sorted feature names\n",
    "\n",
    "#coefs relative weight\n",
    "prop_coefs = (np.abs(sorted_coefs) / np.abs(sorted_coefs).sum()) * 100\n",
    "tresh_coef_idx_80 = np.where(prop_coefs.cumsum()>=80)[0][0] \n",
    "tresh_coef_name_80 = sorted_names[tresh_coef_idx_80]\n",
    "tresh_coef_idx_50 = np.where(prop_coefs.cumsum()>=50)[0][0] \n",
    "tresh_coef_name_50 = sorted_names[tresh_coef_idx_50]\n",
    "\n",
    "\n",
    "## Plot coefs\n",
    "\n",
    "#define fig and subplots\n",
    "fig,axs = plt.subplots(2, 1, figsize=(18,15))\n",
    "\n",
    "#plots font sizes\n",
    "l_font = 15\n",
    "t_font = 20\n",
    "txt_font = 12\n",
    "\n",
    "#plot coefs values\n",
    "axs[0].bar(sorted_names,sorted_coefs,color=['#B3B3B3'])\n",
    "axs[0].plot([-1,121], [0, 0], linestyle=':', color =  'k')\n",
    "axs[0].set_ylabel('Coefficients',fontsize = l_font)\n",
    "axs[0].set_xlabel('Features',fontsize = l_font)\n",
    "axs[0].set_title('Model Coefficients',fontsize = t_font)\n",
    "axs[0].set_ylim([-2, 1.1])\n",
    "axs[0].set_xlim([-1, 121])\n",
    "axs[0].set_xticklabels(sorted_names, rotation=90)\n",
    "\n",
    "\n",
    "#plot feature's importance\n",
    "axs[1].plot(sorted_names,prop_coefs,Marker = 'o',color='#B3B3B3',\n",
    "            markerfacecolor = '#E52123',markeredgecolor = '#E52123')\n",
    "\n",
    "axs[1].plot([tresh_coef_idx_80, tresh_coef_idx_80], [0, 9], linestyle=':', color =  'k')\n",
    "axs[1].text(tresh_coef_idx_80+1, 7.5,\n",
    "              '80%',\n",
    "              fontsize = l_font,\n",
    "              color =  '#B3B3B3')\n",
    "\n",
    "axs[1].plot([tresh_coef_idx_50, tresh_coef_idx_50], [0, 9], linestyle=':', color =  'k')\n",
    "axs[1].text(tresh_coef_idx_50+1, 7.5,\n",
    "              '50%',\n",
    "              fontsize = l_font,\n",
    "              color =  '#B3B3B3')\n",
    "\n",
    "axs[1].set_ylabel('Coefficients Prop',fontsize = l_font)\n",
    "axs[1].set_xlabel('Features',fontsize = l_font)\n",
    "axs[1].set_title('Coefficients Importance',fontsize = t_font)\n",
    "axs[1].set_ylim([-0.2, 9])\n",
    "axs[1].set_xlim([-1, 121])\n",
    "axs[1].set_xticklabels(sorted_names, rotation=90)\n",
    "\n",
    "fig.tight_layout(pad=3.0)       \n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "345d6274",
   "metadata": {},
   "source": [
    "## Feature importance results\n",
    "\n",
    "The first thing to notice is that the importance of the features is quite distributed, there's no feature or small group of features responsible for the entire predictive power of the model. The most important feature represents less than 8% of the sum of all coefficients and we need 13 and 29 features to explain 50 and 80% of the total variance, respectively (bottom plot).\n",
    "\n",
    "Going back to the original features one can see that the model is mainly taking advantage of location information ('LocDept'), the code provided for the search authorization ('Search Authorization Code') and the age of the subjects ('Subject Age'). In the top plot we can also see how different features (e.g. different departmens or different search authorization codes) have coefficients with different signs, indicating their direct or inverse relation with the presence of contraband.\n",
    "\n",
    "For descriptive purposes, below are the features that explain 50% of the total variance and the respective sign of the coefficients:<br>\n",
    "\n",
    "   . Wilton dept (-)<br>\n",
    "   . Brandford dept (-)<br>\n",
    "   . Search Authorization Code I (-)<br>\n",
    "   . Bridgeport dept (-)<br>\n",
    "   . Search Authorization Code N (+)<br>\n",
    "   . Danbury dept(-)<br>\n",
    "   . West Haven dept (-)<br>\n",
    "   . Southington dept (+)<br>\n",
    "   . Vernon dept (+)<br>\n",
    "   . New Haven dept (-)<br>\n",
    "   . StatePolice-Waterbury loc-dept (-)<br>\n",
    "   . Subject Age 15-20(+)<br>\n",
    "   . Search Authorization Code O (+)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8f31d57",
   "metadata": {},
   "source": [
    "# Final Observations \n",
    "\n",
    "The model was fitted in this particular data set and reflects its characteristics. Measures were taken to avoid overfitting (e.g. use of regularization and regularization paramaters set by cross-validation) to guarantee that it would generalize to unseen data (e.g. performance evaluated on held-out data), but these don't totally guarantee that the model will behave in the same way when deployed in a real world scenario. If the context from which new observations are collected is very different from the one where the model was fitted its assumptions might not hold.   \n",
    "\n",
    "\n",
    "Our model relies on a relatively small number of original features, a lot of them connected to the location of the intervention and the department responsible for it. If there's change (social, political, etc.) that alters the geographical pattern of contaband occurreces within the state that would probably harm the model performance. Beaurocratic changes in the nomenclatures used for departments, resasons and codes or changes in the areas that the different sub-departments are responsible for would also be problematic.  \n",
    "\n",
    "\n",
    "Finally, our model was fitted to observations in which vehicles were searched by the police. The service we are being asked for should confirm, or not, that search decision. The fact that, from now on, the police officers know that they can always recur to the service to make a search decisions might loosen their search criterion ( avoiding this responsibility and passing it to us ) changing the underlying data patterns that our model was fitted on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a82ae930",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
